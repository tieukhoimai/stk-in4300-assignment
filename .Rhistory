# Perform bootstrap for ridge regression
set.seed(1)
boot_results <- sapply(lambda_grid, function(lambda) {
ridge_bootstrap(train, lambda, B = 100)
})
# Find the optimal lambda
best_lambda_bootstrap <- lambda_grid[which.min(boot_results)]
print(paste("Best Lambda from Bootstrap:", best_lambda_bootstrap))
# Predict and evaluate on test data
ridge_pred_bootstrap <- predict(cv_ridge, s = best_lambda_bootstrap, newx = x_test)
mse_bootstrap <- mean((ridge_pred_bootstrap - y_test)^2)
rmse_bootstrap <- sqrt(mse_bootstrap)
r2_bootstrap <- 1 - (sum((ridge_pred_bootstrap - y_test)^2) / sum((y_test - mean(y_test))^2))
cat(paste0(
"MSE: ", mse_bootstrap, "\n",
"RMSE (Test): ", rmse_bootstrap, "\n",
"R-squared (Test): ", r2_bootstrap, "\n"
))
# Create comparison data frame
comparison_df <- data.frame(
Lambda = rep(lambda_grid, 2),
MSE = c(cv_ridge$cvm, boot_results),
Method = rep(c("Cross-Validation", "Bootstrap"), each = length(lambda_grid))
)
# Plot the results
ggplot(comparison_df, aes(x = log(Lambda), y = MSE, color = Method)) +
geom_line() +
labs(title = "Ridge Regression: Cross-Validation vs Bootstrap",
x = "Log(Lambda)",
y = "Mean Squared Error") +
theme_minimal()
summary(train)
# Fit GAM with smoothing splines (lower complexity)
gam_model_1 <- gam(LC50 ~ s(TPSA, k = 4) + s(SAacc, k = 4) + s(H050, k = 4) +
s(MLOGP, k = 4) + s(RDCHI, k = 4) + s(GATS1p, k = 4) +
s(nN, k = 4) + s(C040, k = 4), data = train)
# Summarize models
summary(gam_model_1)
# reference: https://stackoverflow.com/questions/67077306/plotting-output-of-gam-model
p_obj <- plot(gam_model_1, residuals = TRUE, pages = 1, scale = 0)
p_obj <- p_obj[[1]] # just one smooth so select the first component
sm_df <- as.data.frame(p_obj[c("x", "se", "fit")])
data_df <- as.data.frame(p_obj[c("raw", "p.resid")])
## plot
ggplot(sm_df, aes(x = x, y = fit)) +
geom_rug(data = data_df, mapping = aes(x = raw, y = NULL),
sides = "b") +
geom_point(data = data_df, mapping = aes(x = raw, y = p.resid)) +
geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
alpha = 0.3) +
geom_line() +
labs(x = p_obj$xlab, y = p_obj$ylab)
gam_pred_1 <- predict(gam_model_1, newdata = test)
gam_mse_1 <- mean((gam_pred_1 - y_test)^2)
gam_rmse_1 <- sqrt(gam_mse_1)
gam_r2_1 <- 1 - (sum((gam_pred_1 - y_test)^2) / sum((y_test - mean(y_test))^2))
cat(paste0(
"MSE (Test): ", gam_mse_1, "\n",
"RMSE (Test): ", gam_rmse_1, "\n",
"R-squared (Test): ", gam_r2_1, "\n"
))
# Fit GAM with smoothing splines (higher complexity)
gam_model_2 <- gam(LC50 ~ s(TPSA, k = 6) + s(SAacc, k = 6) + s(H050, k = 6) +
s(MLOGP, k = 6) + s(RDCHI, k = 6) + s(GATS1p, k = 6) +
s(nN, k = 6) + s(C040, k = 6), data = train)
# Summarize models
summary(gam_model_2)
p_obj_2 <- plot(gam_model_2, residuals = TRUE,  pages = 1, scale = 0)
p_obj_2 <- p_obj_2[[1]] # just one smooth so select the first component
sm_df_2 <- as.data.frame(p_obj_2[c("x", "se", "fit")])
data_df_2 <- as.data.frame(p_obj_2[c("raw", "p.resid")])
## plot
ggplot(sm_df, aes(x = x, y = fit)) +
geom_rug(data = data_df_2, mapping = aes(x = raw, y = NULL),
sides = "b") +
geom_point(data = data_df_2, mapping = aes(x = raw, y = p.resid)) +
geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
alpha = 0.3) +
geom_line() +
labs(x = p_obj_2$xlab, y = p_obj_2$ylab)
gam_pred_2 <- predict(gam_model_2, newdata = test)
gam_mse_2 <- mean((gam_pred_2 - y_test)^2)
gam_rmse_2 <- sqrt(gam_mse_2)
gam_r2_2 <- 1 - (sum((gam_pred_2 - y_test)^2) / sum((y_test - mean(y_test))^2))
cat(paste0(
"MSE: (Test):", gam_mse_2, "\n",
"RMSE (Test): ", gam_rmse_2, "\n",
"R-squared (Test): ", gam_r2_2, "\n"
))
# Fit a regression tree model
tree_model <- rpart(LC50 ~ .,
data = train,
method = "anova",
control = rpart.control(cp = 0.001))
printcp(tree_model)  # Display the cost complexity pruning table
# Prune the tree
optimal_cp <- tree_model$cptable[which.min(tree_model$cptable[,"xerror"]), "CP"]
pruned_tree <- prune(tree_model, cp = optimal_cp)
# Visualize the tree
rpart.plot(pruned_tree, main = "Pruned Regression Tree")
# Predict and evaluate on test data
tree_pred <- predict(pruned_tree, newdata = test)
tree_mse <- mean((tree_pred - y_test)^2)
tree_rmse <- sqrt(tree_mse)
tree_r2 <- 1 - (sum((tree_pred - y_test)^2) / sum((y_test - mean(y_test))^2))
cat(paste0(
"MSE (Test): ", tree_mse, "\n",
"RMSE (Test): ", tree_rmse, "\n",
"R-squared (Test): ", tree_r2, "\n"
))
# Create a comprehensive metrics data frame
all_models_metrics <- data.frame(
Model = c("Original", "Dummy Encoding", "Variable Selection", "Ridge CV", "Ridge Bootstrap", "GAM k=4", "GAM k=6", "Tree"),
MSE = c(mse_test, mse_test_transform_dummy, metrics$MSE[metrics$Model == "Forward AIC"], mse_cv, mse_bootstrap, gam_mse_1,
gam_mse_2, tree_mse),
RMSE = c(rmse_test, rmse_test_transform_dummy, metrics$RMSE[metrics$Model == "Forward AIC"], rmse_cv, rmse_bootstrap, gam_rmse_1,
gam_rmse_2, tree_rmse),
R2 = c(r2_test, r2_test_transform_dummy, metrics$R2[metrics$Model == "Forward AIC"], r2_cv, r2_bootstrap, gam_r2_1,
gam_r2_2, tree_r2)
)
# Print the all models metrics
print(all_models_metrics)
## Visualization of Model Comparisons
# Identify the model with the minimum MSE, RMSE and maximum R-squared
best_mse_model <- all_models_metrics$Model[which.min(all_models_metrics$MSE)]
best_rmse_model <- all_models_metrics$Model[which.min(all_models_metrics$RMSE)]
best_r2_model <- all_models_metrics$Model[which.max(all_models_metrics$R2)]
# Plot MSE across selected models
ggplot(all_models_metrics, aes(x = Model, y = MSE, fill = Model == best_mse_model)) +
geom_bar(stat = "identity") +
scale_fill_manual(values = c("gray", "dodgerblue"), guide = "none") +
theme_minimal() +
labs(title = "MSE Across Selected Models", x = "Model", y = "MSE", fill = "Best Model") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Plot RMSE across selected models
ggplot(all_models_metrics, aes(x = Model, y = RMSE, fill = Model == best_mse_model)) +
geom_bar(stat = "identity") +
scale_fill_manual(values = c("gray", "dodgerblue"), guide = "none") +
theme_minimal() +
labs(title = "RMSE Across Selected Models", x = "Model", y = "RMSE", fill = "Best Model") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Plot RMSE across selected models
ggplot(all_models_metrics, aes(x = Model, y = R2, fill = Model == best_mse_model)) +
geom_bar(stat = "identity") +
scale_fill_manual(values = c("gray", "dodgerblue"), guide = "none") +
theme_minimal() +
labs(title = "R^2 Across Selected Models", x = "Model", y = "R2", fill = "Best Model") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
library(mlbench)
data("PimaIndiansDiabetes2")
data <- PimaIndiansDiabetes2
head(data)
# Checking missing value
sapply(data, function(x) sum(is.na(x)))
# Remove rows with missing values
data <- na.omit(data)
head(data)
summary(data)
# Checking how balance is with the dependent variable
prop.table(table(data$diabetes))
set.seed(123)
sample <- sample.split(data$diabetes, SplitRatio = 2/3)
train <- subset(data, sample == TRUE)
test <- subset(data, sample == FALSE)
# Class distribution in the training set
prop.table(table(train$diabetes))
# Class distribution in the testing set
prop.table(table(test$diabetes))
cat("Dimension of Training Set:", paste(dim(train), collapse = "x"), "\nDimension of Test Set:", paste(dim(test), collapse = "x"), "\n")
library(class)
library(caret)
library(FNN)
X_train <- train[, -ncol(train)]
y_train <- train$diabetes
X_test <- test[, -ncol(test)]
y_test <- test$diabetes
accuracy = function(actual, predicted) {
mean(actual == predicted)
}
set.seed(42)
k_to_try = 1:100
acc_k = rep(0, length(k_to_try))
# Loop over values of k
for (i in seq_along(k_to_try)) {
pred <- knn(
train = scale(X_train),
test = scale(X_test),
cl = y_train,
k = k_to_try[i]
)
acc_k[i] <- accuracy(y_test, pred)
}
ex_seq = seq(from = 1, to = 100, by = 5)
ex_storage = rep(x = 0, times = length(ex_seq))
for(i in seq_along(ex_seq)) {
ex_storage[i] = mean(rnorm(n = 10, mean = ex_seq[i], sd = 1))
}
ex_storage
# Reference: https://daviddalpiaz.github.io/r4sl/k-nearest-neighbors.html
# plot accuracy vs choice of k
plot(acc_k, type = "b", col = "dodgerblue", cex = 1, pch = 20,
xlab = "k, number of neighbors", ylab = "classification accuracy",
main = "Accuracy vs Neighbors")
# add lines indicating k with best accuracy
abline(v = which(acc_k == max(acc_k)), col = "darkorange", lwd = 1.5)
# add line for max accuracy seen
abline(h = max(acc_k), col = "grey", lty = 2)
# add line for prevalence in test set
abline(h = mean(y_test == "No"), col = "grey", lty = 2)
cat("Number of optimal k: ", paste(max(which(acc_k == max(acc_k)))),
"\nAccuracy: ", paste(max(acc_k)))
# Get K-NN result using best k
best_k <- max(which(acc_k == max(acc_k)))
set.seed(42)
knn_pred <- knn(train = scale(X_train), test = scale(X_test), cl = y_train, k = best_k)
knn_accuracy <- accuracy(y_test, knn_pred)
# Convert K-NN predicted labels to probabilities for ROC
knn_pred_prob <- ifelse(knn_pred == "pos", 1, 0)
# Calculate ROC curve for K-NN
roc_curve_knn <- roc(as.numeric(y_test == "pos"), knn_pred_prob)
# Calculate and print AUC for K-NN
auc_knn <- auc(roc_curve_knn)
cat("k-NN\nAUC:", auc_knn,
"\nAccuracy:", knn_accuracy)
# Plot ROC
plot(roc_curve_knn)
# 5-fold cross-validation using caret
train_control <- trainControl(method = "cv", number = 5)
train_knn <- train(diabetes ~ .,
data = train,
method = "knn",
trControl = train_control,
tuneGrid = expand.grid(k = k_to_try))
## plot(train_knn)
cv_results <- train_knn$results
head(cv_results)
# Plot the 5-fold CV accuracy vs choice of k
plot(cv_results$k,
cv_results$Accuracy,
type = "b",
col = "red",
cex = 1,
pch = 20,
xlab = "k, number of neighbors", ylab = "classification accuracy",
main = "5-fold CV Accuracy vs Neighbors")
# Add lines indicating k with best accuracy
abline(v = cv_results$k[which.max(cv_results$Accuracy)],
col = "darkorange",
lwd = 1.5)
# Add line for max accuracy seen
abline(h = max(cv_results$Accuracy), col = "grey", lty = 2)
cat("k-NN (5 fold CV)",
"\nNumber of optimal k: ",
paste(cv_results$k[which.max(cv_results$Accuracy)]),
"\nAccuracy: ",
paste(max(cv_results$Accuracy)))
# Get K-NN result using best k
best_k <- cv_results$k[which.max(cv_results$Accuracy)]
set.seed(42)
knn_5f_pred <- knn(train = scale(X_train),
test = scale(X_test),
cl = y_train,
k = best_k)
# Convert K-NN predicted labels to probabilities for ROC
knn_5f_pred_prob <- ifelse(knn_5f_pred == "pos", 1, 0)
# Calculate ROC curve for K-NN
roc_curve_knn_5f <- roc(as.numeric(y_test == "pos"), knn_pred_prob)
# Calculate and print AUC for K-NN
auc_knn_5f <- auc(roc_curve_knn_5f)
cat("k-NN - AUC:", auc_knn_5f, "\n")
# Plot ROC
plot(roc_curve_knn_5f)
# leave-one-out cross-validation using caret
train_control_loocv <- trainControl(method = "LOOCV")
train_knn_loocv <- train(diabetes ~ .,
data = train,
method = "knn",
trControl = train_control_loocv,
tuneGrid = expand.grid(k = k_to_try))
cv_results_loocv <- train_knn_loocv$results
head(cv_results_loocv)
# Plot the LOOCV accuracy vs choice of k
plot(cv_results_loocv$k,
cv_results_loocv$Accuracy,
type = "b",
col = "red",
cex = 1,
pch = 20,
xlab = "k, number of neighbors",
ylab = "classification accuracy",
main = "LOOCV Accuracy vs Neighbors")
# Add lines indicating k with best accuracy
abline(v = cv_results_loocv$k[which.max(cv_results_loocv$Accuracy)],
col = "darkorange",
lwd = 1.5)
# Add line for max accuracy seen
abline(h = max(cv_results_loocv$Accuracy), col = "grey", lty = 2)
cat("k-NN (LOOCV)",
"\nNumber of optimal k: ",
paste(cv_results_loocv$k[which.max(cv_results_loocv$Accuracy)]),
"\nAccuracy: ",
paste(max(cv_results_loocv$Accuracy)))
# Get K-NN result using best k
best_k_loocv <- cv_results$k[which.max(cv_results_loocv$Accuracy)]
set.seed(42)
knn_loocv_pred <- knn(train = scale(X_train),
test = scale(X_test),
cl = y_train,
k = best_k)
# Convert K-NN predicted labels to probabilities for ROC
knn_loocv_pred_prob <- ifelse(knn_loocv_pred == "pos", 1, 0)
# Calculate ROC curve for K-NN
roc_curve_knn_loocv <- roc(as.numeric(y_test == "pos"), knn_pred_prob)
# Calculate and print AUC for K-NN
auc_knn_loocv <- auc(roc_curve_knn_loocv)
cat("k-NN - AUC:", auc_knn_loocv, "\n")
# Plot ROC
plot(roc_curve_knn_loocv)
# reference: <https://osf.io/wgc4f/wiki/mgcv:%20model%20selection/>
# Fit a GAM with automatic smoothness selection
gam_model <- gam(
diabetes ~
s(glucose) +
s(pressure) +
s(insulin) +
s(mass) +
s(pedigree) +
s(age) +
s(pregnant),
data = train,
family = binomial(link = 'logit'),
select = TRUE,
method= "REML")
summary(gam_model)
p_obj <- plot(gam_model, residuals = TRUE, pages = 1, scale = 0)
p_obj <- p_obj[[1]] # just one smooth so select the first component
sm_df <- as.data.frame(p_obj[c("x", "se", "fit")])
data_df <- as.data.frame(p_obj[c("raw", "p.resid")])
## plot
ggplot(sm_df, aes(x = x, y = fit)) +
geom_rug(data = data_df, mapping = aes(x = raw, y = NULL),
sides = "b") +
geom_point(data = data_df, mapping = aes(x = raw, y = p.resid)) +
geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
alpha = 0.3) +
geom_line() +
labs(x = p_obj$xlab, y = p_obj$ylab)
# Predictions and probabilities for GAM
gam_pred_prob <- predict(gam_model, newdata = test, type = "response")
# Convert probabilities to binary predictions for confusion matrix
gam_pred_class <- ifelse(gam_pred_prob > 0.5, "pos", "neg")
# Calculate accuracy for GAM
gam_accuracy <- mean(gam_pred_class == test$diabetes)
# Calculate ROC curve for GAM
roc_curve_gam <- roc(test$diabetes, gam_pred_prob)
# Calculate and print AUC for GAM
auc_gam <- auc(roc_curve_gam)
cat("GAM\nTest Accuracy:", gam_accuracy)
cat("\nAUC:", auc_gam, "\n")
# Plot ROC
plot(roc_curve_gam)
# reference: https://quantdev.ssri.psu.edu/sites/qdev/files/09_EnsembleMethods_2017_1127.html
set.seed(1234)
# Setting up cross-validation
cvcontrol <- trainControl(method="repeatedcv",
number = 10,
allowParallel=TRUE)
train.tree <- train(as.factor(diabetes) ~ .,
data=train,
method="ctree",
trControl=cvcontrol,
tuneLength = 10)
train.tree
plot(train.tree)
plot(train.tree$finalModel)
# obtaining class predictions for training
tree.classTrain <-  predict(train.tree, type="raw")
# Check accuracy and confusion matrix for training set
confusionMatrix(train$diabetes, tree.classTrain)
# Obtaining class predictions for test set
tree.classTest <-  predict(train.tree,
newdata = test,
type="raw")
# Check accuracy and confusion matrix for training set
confusionMatrix(test$diabetes, tree.classTest)
tree_accuracy <- mean(tree.classTest == test$diabetes)
#Obtaining predicted probabilites for Test data
tree.probs=predict(train.tree,
newdata=test,
type="prob")
#Calculate ROC curve
rocCurve.tree <- roc(test$diabetes,tree.probs[,"neg"])
#plot the ROC curve
plot(rocCurve.tree,col=c(4))
cat("Classification Tree",
"\nAccuracy:", tree_accuracy,
"\nAUC:",auc(rocCurve.tree)
)
train.bagg <- train(as.factor(diabetes) ~ .,
data=train,
method="treebag",
trControl=cvcontrol,
importance=TRUE)
train.bagg
plot(varImp(train.bagg))
#obtaining class predictions
bagg.classTrain <-  predict(train.bagg, type="raw")
confusionMatrix(train$diabetes, bagg.classTrain)
bagg.classTest <-  predict(train.bagg,
newdata = test,
type="raw")
confusionMatrix(test$diabetes, bagg.classTest)
bagg_accuracy <- mean(bagg.classTest == test$diabetes)
#Obtaining predicted probabilities for Test data
bagg.probs=predict(train.bagg,
newdata=test,
type="prob")
#Calculate ROC curve
rocCurve.bagg <- roc(test$diabetes, bagg.probs[,"neg"])
# plot the ROC curve
plot(rocCurve.tree,col=c(4))
cat("Classification Tree",
"\nAccuracy:", bagg_accuracy,
"\nAUC:",auc(rocCurve.bagg)
)
#Using treebag
train.rf <- train(as.factor(diabetes) ~ .,
data=train,
method="rf",
trControl=cvcontrol,
importance=TRUE)
train.rf
#obtaining class predictions
rf.classTrain <-  predict(train.rf, type="raw")
confusionMatrix(train$diabetes, rf.classTrain)
rf.classTest <-  predict(train.rf,
newdata = test,
type="raw")
confusionMatrix(test$diabetes, rf.classTest)
rf_accuracy <- mean(bagg.classTest == test$diabetes)
#Obtaining predicted probabilities for Test data
rf.probs=predict(train.rf,
newdata=test,
type="prob")
#Calculate ROC curve
rocCurve.rf <- roc(test$diabetes, rf.probs[,"neg"])
#calculate the area under curve (bigger is better)
auc(rocCurve.rf)
# plot the ROC curve
plot(rocCurve.tree,col=c(4))
cat("Classification Tree",
"\nAccuracy:", rf_accuracy,
"\nAUC:", auc(rocCurve.rf)
)
library(tidyverse)
library(neuralnet)
# reference: https://www.datacamp.com/tutorial/neural-network-models-r
# Extract features and labels
X_train_nn <- as.data.frame(train[, -ncol(train)])
y_train_nn <- as.numeric(train$diabetes == "pos")  # Convert to binary 0/1
X_test_nn <- as.data.frame(test[, -ncol(test)])
y_test_nn <- as.numeric(test$diabetes == "pos")
# Scale the features
X_train_nn_scaled <- scale(X_train_nn)
X_test_nn_scaled <- scale(X_test_nn)
# Combine scaled features and labels for training
train_combined <- cbind(X_train_nn_scaled, diabetes = y_train_nn)
nn_model = neuralnet(
diabetes~.,
data=train_combined,
hidden=c(16,8,4,2),
linear.output = FALSE
)
plot(nn_model,rep = "best")
# Predict probabilities on test set
nn_pred <- compute(nn_model, X_test_nn_scaled)
nn_pred_prob <- nn_pred$net.result
# Convert probabilities to binary predictions
nn_pred_class <- ifelse(nn_pred_prob > 0.5, 1, 0)
# Calculate and print confusion matrix
conf_matrix <- table(Predicted = nn_pred_class, Actual = as.numeric(test$diabetes == "pos"))
print(conf_matrix)
# Calculate accuracy
check <- as.numeric(test$diabetes == "pos") == nn_pred_class
nn_accuracy <- (sum(check) / nrow(test))
roc_curve_nn <- roc(test$diabetes, as.numeric(nn_pred_prob))
# Calculate and print AUC
auc_nn <- auc(roc_curve_nn)
# Plot the ROC curve
plot(roc_curve_nn, col = "blue", main = "ROC Curve - Neural Network")
cat("Neural Network",
"\nAccuracy:", nn_accuracy,
"\nAUC:", auc_nn
)
# Create a summary table for accuracy and AUC
accuracy_results <- data.frame(
Model = c("KNN", "GAM", "Classification Tree", "Bagged Trees", "Random Forest", "Neural Network"),
Accuracy = c(knn_accuracy, gam_accuracy, tree_accuracy, bagg_accuracy, rf_accuracy, nn_accuracy),
AUC = c(auc_knn, auc_gam, auc(rocCurve.tree), auc(rocCurve.bagg), auc(rocCurve.rf), auc_nn)
)
# Print the summary table
print(accuracy_results)
# Plot ROC curves for ALL models
plot(rocCurve.tree, col = 4, main = "ROC Curve Comparison", lwd = 2)
lines(rocCurve.bagg, col = 6, lwd = 2)  # Add ROC curve for bagged trees
lines(rocCurve.rf, col = 1, lwd = 2)    # Add ROC curve for random forest
lines(roc_curve_nn, col = "blue", lwd = 2)  # Add ROC curve for neural network
lines(roc_curve_knn, col = "green", lwd = 2)  # Add ROC curve for k-NN
lines(roc_curve_gam, col = "purple", lwd = 2)  # Add ROC curve for GAM
# Add a legend to the plot
legend("bottomright", legend = c("Classification Tree", "Bagged Trees", "Random Forest", "Neural Network", "k-NN", "GAM"),
col = c(4, 6, 1, "blue", "green", "purple"), lwd = 2, bty = "n")
