# Predict and evaluate on test data
gam_pred_1 <- predict(gam_model_1, newdata = test)
gam_mse_1 <- mean((gam_pred_1 - y_test)^2)
gam_rmse_1 <- sqrt(gam_mse_1)
gam_r2_1 <- 1 - (sum((gam_pred_1 - y_test)^2) / sum((y_test - mean(y_test))^2))
print(paste("Test MSE for GAM Model 1 (k=4):", gam_mse_1))
print(paste("Test RMSE for GAM Model 1 (k=4):", gam_rmse_1))
print(paste("Test R2 for GAM Model 1 (k=4):", gam_r2_1))
# Fit GAM with smoothing splines (lower complexity)
gam_model_1 <- gam(LC50 ~ s(TPSA, k = 7) + s(SAacc, k = 7) + s(H050, k = 7) +
s(MLOGP, k = 7) + s(RDCHI, k = 7) + s(GATS1p, k = 7) +
s(nN, k = 7) + s(C040, k = 7), data = train)
# Fit a regression tree model
tree_model <- rpart(LC50 ~ ., data = train, method = "anova", control = rpart.control(cp = 0.001))
knitr::opts_chunk$set(echo = TRUE)
library(caTools)
library(ggplot2)
library(gridExtra)
library(MASS)
library(glmnet)
library(boot)
library(mgcv)
library(rpart)
library(rpart.plot)
install.packages('rpart')
install.packages('rpart.plot')
knitr::opts_chunk$set(echo = TRUE)
library(caTools)
library(ggplot2)
library(gridExtra)
library(MASS)
library(glmnet)
library(boot)
library(mgcv)
library(rpart)
library(rpart.plot)
# Fit a regression tree model
tree_model <- rpart(LC50 ~ ., data = train, method = "anova", control = rpart.control(cp = 0.001))
printcp(tree_model)  # Display the cost complexity pruning table
# Prune the tree
optimal_cp <- tree_model$cptable[which.min(tree_model$cptable[,"xerror"]), "CP"]
pruned_tree <- prune(tree_model, cp = optimal_cp)
# Visualize the tree
rpart.plot(pruned_tree, main = "Pruned Regression Tree")
# Predict and evaluate on test data
tree_pred <- predict(pruned_tree, newdata = test)
tree_mse <- mean((tree_pred - y_test)^2)
tree_rmse <- sqrt(tree_mse)
tree_r2 <- 1 - (sum((tree_pred - y_test)^2) / sum((y_test - mean(y_test))^2))
print(paste("Test MSE for Regression Tree:", tree_mse))
print(paste("Test RMSE for Regression Tree:", tree_rmse))
print(paste("Test R2 for Regression Tree:", tree_r2))
# Fit a regression tree model
tree_model <- rpart(LC50 ~ ., data = train, method = "anova", control = rpart.control(cp = 0.001))
printcp(tree_model)  # Display the cost complexity pruning table
# Prune the tree
optimal_cp <- tree_model$cptable[which.min(tree_model$cptable[,"xerror"]), "CP"]
pruned_tree <- prune(tree_model, cp = optimal_cp)
# Visualize the tree
rpart.plot(pruned_tree, main = "Pruned Regression Tree")
# Predict and evaluate on test data
tree_pred <- predict(pruned_tree, newdata = test)
tree_mse <- mean((tree_pred - y_test)^2)
tree_rmse <- sqrt(tree_mse)
tree_r2 <- 1 - (sum((tree_pred - y_test)^2) / sum((y_test - mean(y_test))^2))
print(paste("Test MSE for Regression Tree:", tree_mse))
print(paste("Test RMSE for Regression Tree:", tree_rmse))
print(paste("Test R2 for Regression Tree:", tree_r2))
# Visualize the tree
rpart.plot(pruned_tree, main = "Pruned Regression Tree")
# Fit a regression tree model
tree_model <- rpart(LC50 ~ ., data = train, method = "anova", control = rpart.control(cp = 0.001))
printcp(tree_model)  # Display the cost complexity pruning table
# Prune the tree
optimal_cp <- tree_model$cptable[which.min(tree_model$cptable[,"xerror"]), "CP"]
pruned_tree <- prune(tree_model, cp = optimal_cp)
knitr::opts_chunk$set(echo = TRUE)
library(caTools)
library(ggplot2)
library(gridExtra)
library(MASS)
library(glmnet)
library(boot)
library(mgcv)
library(rpart)
library(rpart.plot)
data <- read.csv("qsar_aquatic_toxicity.csv", sep = ";", header = FALSE)
names(data) <- c(
"TPSA",
"SAacc",
"H050",
"MLOGP",
"RDCHI",
"GATS1p",
"nN",
"C040",
"LC50"
)
head(data)
# Use 70% of dataset as training set and remaining 30% as testing set
set.seed(1)
sample <- sample.split(data$LC50, SplitRatio = 0.7)
train  <- subset(data, sample == TRUE)
test   <- subset(data, sample == FALSE)
cat("Dimension of Training Set:", paste(dim(train), collapse = "x"), "\nDimension of Test Set:", paste(dim(test), collapse = "x"), "\n")
train_i = train
test_i = test
# Fit linear regression model on training data
model <- lm(LC50 ~ ., data=train_i)
summary(model)
# Predict on training and test datasets
pred_train <- predict(model, newdata=train_i)
pred_test <- predict(model, newdata=test_i)
# Adding predictions columns to the datasets
train_i$predicted_LC50 <- pred_train
test_i$predicted_LC50 <- pred_test
# Evaluate model: calculate MSE, RMSE, and R-squared for training and test sets
mse_train <- mean((train_i$LC50 - train_i$predicted_LC50)^2)
rmse_train <- sqrt(mse_train)
r2_train <- 1 - (sum((train_i$LC50 - train_i$predicted_LC50)^2) / sum((train_i$LC50 - mean(train_i$LC50))^2))
mse_test <- mean((test_i$LC50 - test_i$predicted_LC50)^2)
rmse_test <- sqrt(mse_test)
r2_test <- 1 - (sum((test_i$LC50 - test_i$predicted_LC50)^2) / sum((test_i$LC50 - mean(test_i$LC50))^2))
cat(paste0(
"Training Metrics:\n",
"MSE (Train): ", mse_train, "\n",
"RMSE (Train): ", rmse_train, "\n",
"R-squared (Train): ", r2_train, "\n\n",
"Test Metrics:\n",
"MSE (Test): ", mse_test, "\n",
"RMSE (Test): ", rmse_test, "\n",
"R-squared (Test): ", r2_test, "\n"
))
# Combine data for plotting
train_i$Type <- 'Train'
test_i$Type <- 'Test'
combined_data <- rbind(train_i, test_i)
combined_data$Type <- factor(combined_data$Type, levels = c('Train', 'Test'))
# Plotting observed vs predicted LC50 values
ggplot(combined_data, aes(x = LC50, y = predicted_LC50, color = Type)) +
geom_point(alpha = 0.7) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
labs(title = "Observed vs Predicted LC50", x = "Observed LC50", y = "Predicted LC50") +
theme_minimal() +
facet_wrap(~Type) +
theme(legend.position = "bottom")
# To make sure we use the same split in (i)
train_ii = train
test_ii = test
# Transform 3 count variables (H050, nN, C040) into 0/1 in train and test datasets
train_ii$H050 <- ifelse(train_ii$H050 > 0, 1, 0)
train_ii$nN <- ifelse(train_ii$nN > 0, 1, 0)
train_ii$C040 <- ifelse(train_ii$C040 > 0, 1, 0)
test_ii$H050 <- ifelse(test_ii$H050 > 0, 1, 0)
test_ii$nN <- ifelse(test_ii$nN > 0, 1, 0)
test_ii$C040 <- ifelse(test_ii$C040 > 0, 1, 0)
head(train_ii)
# Fit linear regression model on transformed training data
model_transform_dummy <- lm(LC50 ~ ., data = train_ii)
summary(model_transform_dummy)
# Predict on training and test datasets
pred_train_transform_dummy <- predict(model, newdata=train_ii)
pred_test_transform_dummy <- predict(model, newdata=test_ii)
# Adding predictions columns to the datasets
train_ii$predicted_LC50 <- pred_train_transform_dummy
test_ii$predicted_LC50 <- pred_test_transform_dummy
# Evaluate model: calculate MSE, RMSE, and R-squared for training and test sets
mse_train_transform_dummy <- mean((train_ii$LC50 - train_ii$predicted_LC50)^2)
rmse_train_transform_dummy <- sqrt(mse_train_transform_dummy)
r2_train_transform_dummy <- 1 - (sum((train_ii$LC50 - train_ii$predicted_LC50)^2) / sum((train_ii$LC50 - mean(train_ii$LC50))^2))
mse_test_transform_dummy <- mean((test_ii$LC50 - test_ii$predicted_LC50)^2)
rmse_test_transform_dummy <- sqrt(mse_test_transform_dummy)
r2_test_transform_dummy <- 1 - (sum((test_ii$LC50 - test_ii$predicted_LC50)^2) / sum((test_ii$LC50 - mean(test_ii$LC50))^2))
cat(paste0(
"Training Metrics:\n",
"MSE (Train): ", mse_train_transform_dummy, "\n",
"RMSE (Train): ", rmse_train_transform_dummy, "\n",
"R-squared (Train): ", r2_train_transform_dummy, "\n\n",
"Test Metrics:\n",
"MSE (Test): ", mse_test_transform_dummy, "\n",
"RMSE (Test): ", rmse_test_transform_dummy, "\n",
"R-squared (Test): ", r2_test_transform_dummy, "\n"
))
# Combine data for plotting
train_ii$Type <- 'Train'
test_ii$Type <- 'Test'
combined_data <- rbind(train_ii, test_ii)
combined_data$Type <- factor(combined_data$Type, levels = c('Train', 'Test'))
# Plotting observed vs predicted LC50 values
ggplot(combined_data, aes(x = LC50, y = predicted_LC50, color = Type)) +
geom_point(alpha = 0.7) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
labs(title = "Dummy Encoding: Observed vs Predicted LC50", x = "Observed LC50", y = "Predicted LC50") +
theme_minimal() +
facet_wrap(~Type) +
theme(legend.position = "bottom")
# Prepare combined data
train_combined <- train_i[, c("LC50", "predicted_LC50")]
train_combined$Method <- 'Original'
train_combined$Type <- 'Train'
train_ii_combined <- train_ii[, c("LC50", "predicted_LC50")]
train_ii_combined$Method <- 'Dummy'
train_ii_combined$Type <- 'Train'
train_combined_all <- rbind(train_combined, train_ii_combined)
test_combined <- test_i[, c("LC50", "predicted_LC50")]
test_combined$Method <- 'Original'
test_combined$Type <- 'Test'
test_ii_combined <- test_ii[, c("LC50", "predicted_LC50")]
test_ii_combined$Method <- 'Dummy'
test_ii_combined$Type <- 'Test'
test_combined_all <- rbind(test_combined, test_ii_combined)
# Convert 'Method' and 'Type' to factors
train_combined_all$Method <- factor(train_combined_all$Method, levels = c('Original', 'Dummy'))
test_combined_all$Method <- factor(test_combined_all$Method, levels = c('Original', 'Dummy'))
# Function to draw regression lines
add_regression_lines <- function(df, original_model, dummy_model) {
ggplot(df, aes(x = LC50, y = predicted_LC50, color = Method)) +
geom_point(alpha = 0.7) +
geom_smooth(method = "lm", formula = y ~ x, se = FALSE,
aes(linetype = Method),
data = df[df$Method == 'Original', ],
color = 'blue') +
geom_smooth(method = "lm", formula = y ~ x, se = FALSE,
aes(linetype = Method),
data = df[df$Method == 'Dummy', ],
color = 'red') +
geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
labs(x = "Observed LC50", y = "Predicted LC50", title = df$Type[1]) +
theme_minimal() +
theme(legend.position = "bottom")
}
# Plot training data with both regression lines
train_plot <- add_regression_lines(train_combined_all, model, model_transform_dummy)
train_plot <- train_plot + labs(title = "Training Data")
# Plot testing data with both regression lines
test_plot <- add_regression_lines(test_combined_all, model, model_transform_dummy)
test_plot <- test_plot + labs(title = "Testing Data")
# Display plots side by side
grid.arrange(train_plot, test_plot, ncol = 2)
# Initialize vectors to store test errors
mse_test_errors_i <- numeric(200)
rmse_test_errors_i <- numeric(200)
r2_test_errors_i <- numeric(200)
mse_test_errors_ii <- numeric(200)
rmse_test_errors_ii <- numeric(200)
r2_test_errors_ii <- numeric(200)
# Repeat the procedure 200 times
set.seed(2)
for (i in 1:200) {
# Split the data
sample <- sample.split(data$LC50, SplitRatio = 0.7)
train <- subset(data, sample == TRUE)
test <- subset(data, sample == FALSE)
# Option (i): Original model
model <- lm(LC50 ~ ., data=train)
pred_test_i <- predict(model, newdata=test)
mse_test_i <- mean((test$LC50 - pred_test_i)^2)
rmse_test_i <- sqrt(mse_test_i)
r2_test_i <- 1 - (sum((test$LC50 - pred_test_i)^2) / sum((test$LC50 - mean(test$LC50))^2))
# Option (ii): Dummy encoding
train$H050 <- ifelse(train$H050 > 0, 1, 0)
train$nN <- ifelse(train$nN > 0, 1, 0)
train$C040 <- ifelse(train$C040 > 0, 1, 0)
test$H050 <- ifelse(test$H050 > 0, 1, 0)
test$nN <- ifelse(test$nN > 0, 1, 0)
test$C040 <- ifelse(test$C040 > 0, 1, 0)
model_ii <- lm(LC50 ~ ., data = train)
pred_test_ii <- predict(model_ii, newdata = test)
mse_test_ii <- mean((test$LC50 - pred_test_ii)^2)
rmse_test_ii <- sqrt(mse_test_ii)
r2_test_ii <- 1 - (sum((test$LC50 - pred_test_ii)^2) / sum((test$LC50 - mean(test$LC50))^2))
# Record the test errors
mse_test_errors_i[i] <- mse_test_i
rmse_test_errors_i[i] <- rmse_test_i
r2_test_errors_i[i] <- r2_test_i
mse_test_errors_ii[i] <- mse_test_ii
rmse_test_errors_ii[i] <- rmse_test_ii
r2_test_errors_ii[i] <- r2_test_ii
}
# Calculate and print average test errors
average_test_error_i <- mean(mse_test_errors_i)
average_rmse_error_i <- mean(rmse_test_errors_i)
average_r2_error_i <- mean(r2_test_errors_i)
average_test_error_ii <- mean(mse_test_errors_ii)
average_rmse_error_ii <- mean(rmse_test_errors_ii)
average_r2_error_ii <- mean(r2_test_errors_ii)
cat(paste0(
"Average Test Errors (Original Model):\n",
"MSE: ", average_test_error_i, "\n",
"RMSE: ", average_rmse_error_i, "\n",
"R-squared: ", average_r2_error_i, "\n\n",
"Average Test Errors (Dummy Model):\n",
"MSE: ", average_test_error_ii, "\n",
"RMSE: ", average_rmse_error_ii, "\n",
"R-squared: ", average_r2_error_ii, "\n"
))
# Create data frames for plotting
errors_df_mse <- data.frame(
Error = c(mse_test_errors_i, mse_test_errors_ii),
Metric = 'MSE',
Model = factor(rep(c("Original", "Dummy"), each = 200))
)
errors_df_rmse <- data.frame(
Error = c(rmse_test_errors_i, rmse_test_errors_ii),
Metric = 'RMSE',
Model = factor(rep(c("Original", "Dummy"), each = 200))
)
errors_df_r2 <- data.frame(
Error = c(r2_test_errors_i, r2_test_errors_ii),
Metric = 'R-squared',
Model = factor(rep(c("Original", "Dummy"), each = 200))
)
errors_df <- rbind(errors_df_mse, errors_df_rmse, errors_df_r2)
# Ensure the 'Metric' factor has the correct level order
errors_df$Metric <- factor(errors_df$Metric, levels = c('MSE', 'RMSE', 'R-squared'))
# Plot the empirical distributions of the test errors
ggplot(errors_df, aes(x = Error, fill = Model)) +
geom_density(alpha = 0.5) +
facet_wrap(~ Metric, scales = "free") +
labs(title = "Empirical Distributions of Test Errors", x = "Test Error", y = "Density") +
theme_minimal()
# Plot the empirical distributions of the test errors using boxplots
ggplot(errors_df, aes(x = Metric, y = Error, fill = Model)) +
geom_boxplot(alpha = 0.7) +
labs(title = "Boxplots of Test Errors", x = "Error Metric", y = "Error Value") +
theme_minimal() +
theme(legend.position = "top")
# Split the data into training (70%) and test (30%) sets
set.seed(1)
sample <- sample.split(data$LC50, SplitRatio = 0.7)
train <- subset(data, sample == TRUE)
test <- subset(data, sample == FALSE)
# Set up full and null model
full.model <- lm(LC50 ~ ., data = train)
null.model <- lm(LC50 ~ 1, data = train)
# Set up target and number of variables
y <- train$LC50
num_vars <- ncol(train) - 1  # exclude the response variable column
# With AIC
model.forward.aic <- stepAIC(null.model, scope = list(lower = null.model, upper = full.model), direction = 'forward', trace = FALSE)
summary(model.forward.aic)
# With BIC
# If we set it to k = log(n), the function considers the BIC.
model.forward.bic <- stepAIC(null.model, scope = list(lower = null.model, upper = full.model), direction = 'forward', k = log(nrow(train)), trace = FALSE)
summary(model.forward.bic)
# With AIC
model.backward.aic <- stepAIC(full.model, direction = 'backward', trace = FALSE)
summary(model.backward.aic)
# With BIC
model.backward.bic <- stepAIC(full.model, direction = 'backward', k = log(nrow(train)), trace = FALSE)
summary(model.backward.bic)
# With AIC
model.stepwise.aic <- stepAIC(null.model, scope = list(lower = null.model, upper = full.model), direction = 'both', trace = FALSE)
summary(model.stepwise.aic)
# With BIC
model.stepwise.bic <- stepAIC(null.model, scope = list(lower = null.model, upper = full.model), direction = 'both', k = log(nrow(train)), trace = FALSE)
summary(model.stepwise.bic)
# Predict on the test set using all models
test$pred_backward_aic <- predict(model.backward.aic, newdata = test)
test$pred_forward_aic <- predict(model.forward.aic, newdata = test)
test$pred_stepwise_aic <- predict(model.stepwise.aic, newdata = test)
test$pred_backward_bic <- predict(model.backward.bic, newdata = test)
test$pred_forward_bic <- predict(model.forward.bic, newdata = test)
test$pred_stepwise_bic <- predict(model.stepwise.bic, newdata = test)
# Calculate MSE, RMSE, and R-squared for each model
mse <- function(actual, predicted) mean((actual - predicted)^2)
rmse <- function(actual, predicted) sqrt(mse(actual, predicted))
r2 <- function(actual, predicted) 1 - (sum((actual - predicted)^2) / sum((actual - mean(actual))^2))
metrics <- data.frame(
Model = c("Backward AIC", "Forward AIC", "Stepwise AIC", "Backward BIC", "Forward BIC", "Stepwise BIC"),
MSE = c(
mse(test$LC50, test$pred_backward_aic),
mse(test$LC50, test$pred_forward_aic),
mse(test$LC50, test$pred_stepwise_aic),
mse(test$LC50, test$pred_backward_bic),
mse(test$LC50, test$pred_forward_bic),
mse(test$LC50, test$pred_stepwise_bic)
),
RMSE = c(
rmse(test$LC50, test$pred_backward_aic),
rmse(test$LC50, test$pred_forward_aic),
rmse(test$LC50, test$pred_stepwise_aic),
rmse(test$LC50, test$pred_backward_bic),
rmse(test$LC50, test$pred_forward_bic),
rmse(test$LC50, test$pred_stepwise_bic)
),
R2 = c(
r2(test$LC50, test$pred_backward_aic),
r2(test$LC50, test$pred_forward_aic),
r2(test$LC50, test$pred_stepwise_aic),
r2(test$LC50, test$pred_backward_bic),
r2(test$LC50, test$pred_forward_bic),
r2(test$LC50, test$pred_stepwise_bic)
)
)
print(metrics)
set.seed(1)
sample <- sample.split(data$LC50, SplitRatio = 0.7)
train <- subset(data, sample == TRUE)
test <- subset(data, sample == FALSE)
x_train <- as.matrix(train[, -9])
y_train <- train$LC50
x_test <- as.matrix(test[, -9])
y_test <- test$LC50
# Define a grid of candidate lambda values
lambda_grid <- 10^seq(10, -2, length = 100)
# Perform cross-validation for ridge regression
cv_ridge <- cv.glmnet(x_train, y_train, alpha = 0, lambda = lambda_grid, standardize = TRUE)
best_lambda_cv <- cv_ridge$lambda.min
print(paste("Best Lambda from Cross-Validation:", best_lambda_cv))
# Predict and evaluate on test data
ridge_pred_cv <- predict(cv_ridge, s = best_lambda_cv, newx = x_test)
mse_cv <- mean((ridge_pred_cv - y_test)^2)
print(paste("Test MSE for Ridge Regression (Cross-Validation):", mse_cv))
# Define ridge regression function for bootstrap
ridge_bootstrap <- function(dataset, indices, lambda) {
d <- dataset[indices, ]  # subset data
x_boot <- as.matrix(d[, -9])
y_boot <- d$LC50
ridge_model <- glmnet(x_boot, y_boot, alpha = 0, lambda = lambda, standardize = TRUE)
return(ridge_model)
}
# Bootstrap procedure
set.seed(1)
num_bootstraps <- 100
boot_results <- sapply(lambda_grid, function(lambda) {
boot_mses <- replicate(num_bootstraps, {
sample_indices <- sample(1:nrow(train), replace = TRUE)
ridge_model <- ridge_bootstrap(train, sample_indices, lambda)
boot_pred <- predict(ridge_model, s = lambda, newx = x_test)
mean((boot_pred - y_test)^2)
})
mean(boot_mses)
})
# Find the optimal lambda
best_lambda_bootstrap <- lambda_grid[which.min(boot_results)]
print(paste("Best Lambda from Bootstrap:", best_lambda_bootstrap))
# Predict and evaluate on test data
ridge_pred_bootstrap <- predict(cv_ridge, s = best_lambda_bootstrap, newx = x_test)
mse_bootstrap <- mean((ridge_pred_bootstrap - y_test)^2)
print(paste("Test MSE for Ridge Regression (Bootstrap):", mse_bootstrap))
# Create comparison data frame
comparison_df <- data.frame(
Lambda = rep(lambda_grid, 2),
MSE = c(cv_ridge$cvm, boot_results),
Method = rep(c("Cross-Validation", "Bootstrap"), each = length(lambda_grid))
)
# Plot the results
ggplot(comparison_df, aes(x = log(Lambda), y = MSE, color = Method)) +
geom_line() +
labs(title = "Ridge Regression: Cross-Validation vs Bootstrap", x = "Log(Lambda)", y = "Mean Squared Error") +
theme_minimal()
# Fit GAM with smoothing splines (lower complexity)
gam_model_1 <- gam(LC50 ~ s(TPSA, k = 4) + s(SAacc, k = 4) + s(H050, k = 4) +
s(MLOGP, k = 4) + s(RDCHI, k = 4) + s(GATS1p, k = 4) +
s(nN, k = 4) + s(C040, k = 4), data = train)
# Summarize models
summary(gam_model_1)
# Predict and evaluate on test data
gam_pred_1 <- predict(gam_model_1, newdata = test)
gam_mse_1 <- mean((gam_pred_1 - y_test)^2)
gam_rmse_1 <- sqrt(gam_mse_1)
gam_r2_1 <- 1 - (sum((gam_pred_1 - y_test)^2) / sum((y_test - mean(y_test))^2))
print(paste("Test MSE for GAM Model 1 (k=4):", gam_mse_1))
print(paste("Test RMSE for GAM Model 1 (k=4):", gam_rmse_1))
print(paste("Test R2 for GAM Model 1 (k=4):", gam_r2_1))
# Fit a regression tree model
tree_model <- rpart(LC50 ~ ., data = train, method = "anova", control = rpart.control(cp = 0.001))
printcp(tree_model)  # Display the cost complexity pruning table
# Prune the tree
optimal_cp <- tree_model$cptable[which.min(tree_model$cptable[,"xerror"]), "CP"]
pruned_tree <- prune(tree_model, cp = optimal_cp)
# Visualize the tree
rpart.plot(pruned_tree, main = "Pruned Regression Tree")
# Predict and evaluate on test data
tree_pred <- predict(pruned_tree, newdata = test)
tree_mse <- mean((tree_pred - y_test)^2)
tree_rmse <- sqrt(tree_mse)
tree_r2 <- 1 - (sum((tree_pred - y_test)^2) / sum((y_test - mean(y_test))^2))
print(paste("Test MSE for Regression Tree:", tree_mse))
print(paste("Test RMSE for Regression Tree:", tree_rmse))
print(paste("Test R2 for Regression Tree:", tree_r2))
# Predict and evaluate on test data
tree_pred <- predict(pruned_tree, newdata = test)
tree_mse <- mean((tree_pred - y_test)^2)
tree_rmse <- sqrt(tree_mse)
tree_r2 <- 1 - (sum((tree_pred - y_test)^2) / sum((y_test - mean(y_test))^2))
cat(paste0(
"MSE: ", tree_mse, "\n",
"RMSE (Test): ", tree_rmse, "\n",
"R-squared (Test): ", tree_r2, "\n"
))
# Fit GAM with smoothing splines (lower complexity)
gam_model_1 <- gam(LC50 ~ s(TPSA, k = 4) + s(SAacc, k = 4) + s(H050, k = 4) +
s(MLOGP, k = 4) + s(RDCHI, k = 4) + s(GATS1p, k = 4) +
s(nN, k = 4) + s(C040, k = 4), data = train)
# Summarize models
summary(gam_model_1)
# Predict and evaluate on test data
gam_pred_1 <- predict(gam_model_1, newdata = test)
gam_mse_1 <- mean((gam_pred_1 - y_test)^2)
gam_rmse_1 <- sqrt(gam_mse_1)
gam_r2_1 <- 1 - (sum((gam_pred_1 - y_test)^2) / sum((y_test - mean(y_test))^2))
gam_pred_1 <- predict(gam_model_1, newdata = test)
gam_mse_1 <- mean((gam_pred_1 - y_test)^2)
gam_rmse_1 <- sqrt(gam_mse_1)
gam_r2_1 <- 1 - (sum((gam_pred_1 - y_test)^2) / sum((y_test - mean(y_test))^2))
cat(paste0(
"MSE: ", gam_mse_1, "\n",
"RMSE (Test): ", gam_rmse_1, "\n",
"R-squared (Test): ", gam_r2_1, "\n"
))
